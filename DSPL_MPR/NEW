import pandas as pd
import numpy as np
from math import log2

# Load dataset
df = pd.read_csv('Buy_Computer.csv')

# Function to calculate entropy
def entropy(df, target_column):
    value_counts = df[target_column].value_counts()
    total_records = len(df)
    entropy_value = 0
    for count in value_counts:
        prob = count / total_records
        entropy_value -= prob * log2(prob)
    return entropy_value

# Function to calculate information gain
def information_gain(df, attribute, target_column):
    total_entropy = entropy(df, target_column)
    attribute_values = df[attribute].value_counts()
    weighted_entropy = 0
    for value, count in attribute_values.items():
        subset = df[df[attribute] == value]
        weighted_entropy += (count / len(df)) * entropy(subset, target_column)
    return total_entropy - weighted_entropy

# First iteration
target_column = 'Buy_Computer'
attributes = ['age', 'income', 'student', 'credit_rating']

# Calculate Information Gain for each attribute
information_gains = {}
for attribute in attributes:
    information_gains[attribute] = information_gain(df, attribute, target_column)

# Find the best attribute to split on
best_attribute = max(information_gains, key=information_gains.get)

# Split based on the best attribute and print subsets
subsets = {}
for value in df[best_attribute].unique():
    subset = df[df[best_attribute] == value]
    subsets[value] = subset

print("Information Gain for each attribute (First Iteration):")
for attribute, gain in information_gains.items():
    print(f"{attribute}: {gain}")
    
print(f"\nBest attribute to split on: {best_attribute}")
print("\nThe dataset is split based on the attribute:", best_attribute)

# Second iteration for youth and senior subsets
print("\nSecond Iteration for youth and senior subsets:")
for value in ['youth', 'senior']:
    subset = subsets[value]
    print(f"\nSecond Iteration for subset where {best_attribute} = {value}:")

    remaining_attributes = [attr for attr in attributes if attr != best_attribute]

    information_gains_2 = {}
    for attribute in remaining_attributes:
        information_gains_2[attribute] = information_gain(subset, attribute, target_column)

    best_attribute_2 = max(information_gains_2, key=information_gains_2.get)

    print(f"\nInformation Gain for each attribute in {value} subset:")
    for attribute, gain in information_gains_2.items():
        print(f"{attribute}: {gain}")
        
    print(f"\nBest attribute to split on in this subset: {best_attribute_2}")
    print(f"\nRule for splitting {value} subset: Split by {best_attribute_2}")
    print(f"\nThe dataset is split based on the attribute: {best_attribute_2}")

    for val in subset[best_attribute_2].unique():
        subset_split = subset[subset[best_attribute_2] == val]
        print(f"\nSubset where {best_attribute_2} = {val}:")
        print(subset_split)

        rule_result = subset_split[target_column].iloc[0]
        print(f"\nRule for {best_attribute_2} = {val}: If {best_attribute_2} = {val}, then {target_column} = {rule_result}")

# Check if a subset is pure (only one class)
def check_pure_subset(df, target_column):
    return len(df[target_column].unique()) == 1

# Store tree structure
decision_tree = {}

# Recursive function to build the tree
def build_tree(subset, parent_attribute="Root"):
    if check_pure_subset(subset, target_column):
        decision_tree[parent_attribute] = subset[target_column].iloc[0]
        return

    remaining_attributes = [attr for attr in attributes if attr not in decision_tree.values()]
    if not remaining_attributes:
        return

    information_gains_local = {attr: information_gain(subset, attr, target_column) for attr in remaining_attributes}
    best_attr = max(information_gains_local, key=information_gains_local.get)
    decision_tree[parent_attribute] = best_attr

    for val in subset[best_attr].unique():
        subset_split = subset[subset[best_attr] == val]
        build_tree(subset_split, f"{best_attr} = {val}")

# Build the tree
for value, subset in subsets.items():
    build_tree(subset, f"{best_attribute} = {value}")

# Print the decision tree
print("\nDecision Tree Representation:")
for key, value in decision_tree.items():
    print(f"{key} -> {value}")
